{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#microphone check one two what is this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from pathlib import Path\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX #pip install statsmodels\n",
    "from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"openpyxl\")\n",
    "\n",
    "#from thefuzz import process - not needed anymore\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import packaging as p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'#check counts and names\\nimport pandas as pd, re\\nfrom pprint import pprint\\n\\nxl = pd.ExcelFile(\"multi-year-station-entry-and-exit-figures.xlsx\")\\n\\ndef norm(s): \\n    import re\\n    s = str(s).lower().strip()\\n    s = re.sub(r\"[^a-z0-9]+\", \" \", s)\\n    return re.sub(r\"\\\\s+\", \" \", s).strip()\\n\\ndef find_header_row(xl, sheet, scan_rows=60):\\n    raw = xl.parse(sheet, header=None)\\n    for i in range(min(scan_rows, len(raw))):\\n        row = [norm(v) for v in raw.iloc[i].tolist()]\\n        if any(\"station\" in v for v in row) or (any(\"entr\" in v for v in row) and any(\"exit\" in v for v in row)):\\n            return i\\n    return None\\n\\ns = \"2017 Entry & Exit\"\\nhdr = find_header_row(xl, s)\\ndf = xl.parse(s, header=hdr).dropna(how=\"all\").dropna(axis=1, how=\"all\")\\npprint([norm(c) for c in df.columns])'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''#check counts and names\n",
    "import pandas as pd, re\n",
    "from pprint import pprint\n",
    "\n",
    "xl = pd.ExcelFile(\"multi-year-station-entry-and-exit-figures.xlsx\")\n",
    "\n",
    "def norm(s): \n",
    "    import re\n",
    "    s = str(s).lower().strip()\n",
    "    s = re.sub(r\"[^a-z0-9]+\", \" \", s)\n",
    "    return re.sub(r\"\\s+\", \" \", s).strip()\n",
    "\n",
    "def find_header_row(xl, sheet, scan_rows=60):\n",
    "    raw = xl.parse(sheet, header=None)\n",
    "    for i in range(min(scan_rows, len(raw))):\n",
    "        row = [norm(v) for v in raw.iloc[i].tolist()]\n",
    "        if any(\"station\" in v for v in row) or (any(\"entr\" in v for v in row) and any(\"exit\" in v for v in row)):\n",
    "            return i\n",
    "    return None\n",
    "\n",
    "s = \"2017 Entry & Exit\"\n",
    "hdr = find_header_row(xl, s)\n",
    "df = xl.parse(s, header=hdr).dropna(how=\"all\").dropna(axis=1, how=\"all\")\n",
    "pprint([norm(c) for c in df.columns])'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote data/processed/station_flow_clean_basic.csv  rows=2968\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>station</th>\n",
       "      <th>annual_entries_exits</th>\n",
       "      <th>daily_avg_passengers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017</td>\n",
       "      <td>Nan</td>\n",
       "      <td>253.0</td>\n",
       "      <td>0.693151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017</td>\n",
       "      <td>625</td>\n",
       "      <td>149150.0</td>\n",
       "      <td>408.630137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017</td>\n",
       "      <td>747</td>\n",
       "      <td>147574.0</td>\n",
       "      <td>404.312329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017</td>\n",
       "      <td>669</td>\n",
       "      <td>121364.0</td>\n",
       "      <td>332.504110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017</td>\n",
       "      <td>741</td>\n",
       "      <td>121018.0</td>\n",
       "      <td>331.556164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2017</td>\n",
       "      <td>635</td>\n",
       "      <td>113606.0</td>\n",
       "      <td>311.249315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2017</td>\n",
       "      <td>634</td>\n",
       "      <td>112437.0</td>\n",
       "      <td>308.046575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2017</td>\n",
       "      <td>719</td>\n",
       "      <td>93742.0</td>\n",
       "      <td>256.827397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2017</td>\n",
       "      <td>513</td>\n",
       "      <td>112547.0</td>\n",
       "      <td>308.347945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2017</td>\n",
       "      <td>852</td>\n",
       "      <td>86663.0</td>\n",
       "      <td>237.432877</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   year station  annual_entries_exits  daily_avg_passengers\n",
       "0  2017     Nan                 253.0              0.693151\n",
       "1  2017     625              149150.0            408.630137\n",
       "2  2017     747              147574.0            404.312329\n",
       "3  2017     669              121364.0            332.504110\n",
       "4  2017     741              121018.0            331.556164\n",
       "5  2017     635              113606.0            311.249315\n",
       "6  2017     634              112437.0            308.046575\n",
       "7  2017     719               93742.0            256.827397\n",
       "8  2017     513              112547.0            308.347945\n",
       "9  2017     852               86663.0            237.432877"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote data/processed/TfL_stations_clean.csv  rows=436\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>station</th>\n",
       "      <th>line</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Acton Town</td>\n",
       "      <td>District, Piccadilly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Barbican</td>\n",
       "      <td>Circle, Hammersmith &amp; City, Metropolitan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Aldgate</td>\n",
       "      <td>Circle, Metropolitan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Aldgate East</td>\n",
       "      <td>District, Hammersmith &amp; City</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Alperton</td>\n",
       "      <td>Piccadilly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Amersham</td>\n",
       "      <td>Metropolitan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Angel</td>\n",
       "      <td>Northern</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Archway</td>\n",
       "      <td>Northern</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Arnos Grove</td>\n",
       "      <td>Piccadilly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Arsenal</td>\n",
       "      <td>Piccadilly</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        station                                      line\n",
       "0    Acton Town                      District, Piccadilly\n",
       "1      Barbican  Circle, Hammersmith & City, Metropolitan\n",
       "2       Aldgate                      Circle, Metropolitan\n",
       "3  Aldgate East              District, Hammersmith & City\n",
       "4      Alperton                                Piccadilly\n",
       "5      Amersham                              Metropolitan\n",
       "6         Angel                                  Northern\n",
       "7       Archway                                  Northern\n",
       "8   Arnos Grove                                Piccadilly\n",
       "9       Arsenal                                Piccadilly"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote data/processed/tube_performance_clean_basic.csv  rows=0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>line</th>\n",
       "      <th>date</th>\n",
       "      <th>metric</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [line, date, metric]\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#paths\n",
    "RAW_STATION_XLSX   = \"multi-year-station-entry-and-exit-figures.xlsx\"\n",
    "RAW_STATIONS_CSV   = \"TfL_stations.csv\"\n",
    "RAW_PERF_XLSX      = \"tfl-tube-performance.xlsx\"\n",
    "\n",
    "OUT_DIR            = \"data/processed\"\n",
    "OUT_STATION_FLOW   = f\"{OUT_DIR}/station_flow_clean_basic.csv\"\n",
    "OUT_STATIONS_CLEAN = f\"{OUT_DIR}/TfL_stations_clean.csv\"\n",
    "OUT_PERF_CLEAN     = f\"{OUT_DIR}/tube_performance_clean_basic.csv\"\n",
    "\n",
    "Path(OUT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "#cleanin file #1:\n",
    "def clean_station_workbook(xlsx_path: str) -> pd.DataFrame:\n",
    "    xl = pd.ExcelFile(xlsx_path)\n",
    "    frames = []\n",
    "    \n",
    "    target_sheets = [s for s in xl.sheet_names if (\"entry\" in s.lower() and \"exit\" in s.lower())]\n",
    "    for s in target_sheets:\n",
    "        raw = xl.parse(s, header=None) #raw read+ drop empty rows/cols\n",
    "        raw = raw.dropna(how=\"all\").dropna(axis=1, how=\"all\")\n",
    "        if raw.empty:\n",
    "            continue\n",
    "\n",
    "        df = raw.copy()\n",
    "        df.columns = [f\"col{i}\" for i in range(df.shape[1])] #assume first column holds station names\n",
    "        \n",
    "        for c in df.columns[1:]:\n",
    "            df[c] = pd.to_numeric(df[c], errors=\"coerce\") #coerce non first columns to numeric\n",
    "\n",
    "        num_cols = [c for c in df.columns[1:] if np.issubdtype(df[c].dtype, np.number)]\n",
    "        if not num_cols:\n",
    "            continue\n",
    "        q90 = {c: df[c].quantile(0.90) for c in num_cols}\n",
    "        total_col = max(q90, key=lambda k: (q90[k] if pd.notna(q90[k]) else -np.inf)) #choose total as numeric column with highest 90th %\n",
    "\n",
    "        out = pd.DataFrame({\n",
    "            \"station\": df[\"col0\"].astype(str).str.strip().str.title(),\n",
    "            \"annual_entries_exits\": df[total_col]})\n",
    "\n",
    "        out = out.replace({\"\": np.nan, \"nan\": np.nan})\n",
    "        out = out[~out[\"station\"].str.contains(r\"^total\\b|^counts\\b|^grand\\b\", case=False, na=False)]\n",
    "        out = out.dropna(subset=[\"station\", \"annual_entries_exits\"]) #drop junk\n",
    "\n",
    "        if out[\"annual_entries_exits\"].max(skipna=True) < 100_000:\n",
    "            out[\"annual_entries_exits\"] = out[\"annual_entries_exits\"] * 1_000_000 #millions to absolute\n",
    "            \n",
    "        out[\"daily_avg_passengers\"] = out[\"annual_entries_exits\"] / 365.0 #daily average\n",
    "\n",
    "        m = re.search(r\"(20\\d{2})\", s)\n",
    "        out[\"year\"] = int(m.group(1)) if m else np.nan\n",
    "        \n",
    "        frames.append(out[[\"year\",\"station\",\"annual_entries_exits\",\"daily_avg_passengers\"]])\n",
    "\n",
    "    if not frames:\n",
    "        raise RuntimeError(\"Station workbook: produced no rows. Check file path or sheet names.\")\n",
    "\n",
    "    station_flow = pd.concat(frames, ignore_index=True).drop_duplicates()\n",
    "    return station_flow\n",
    "\n",
    "station_flow = clean_station_workbook(RAW_STATION_XLSX)\n",
    "station_flow.to_csv(OUT_STATION_FLOW, index=False)\n",
    "print(f\"Wrote {OUT_STATION_FLOW}  rows={len(station_flow)}\")\n",
    "display(station_flow.head(10))\n",
    "\n",
    "\n",
    "#cleanin file #2:\n",
    "def clean_stations_csv(csv_path: str) -> pd.DataFrame:\n",
    "    df = pd.read_csv(csv_path)\n",
    "    df.columns = [re.sub(r\"_+\", \"_\", re.sub(r\"[^0-9a-zA-Z]+\", \"_\", c)).strip(\"_\").lower() for c in df.columns] #normalize headers\n",
    "    \n",
    "    rename_map = {}\n",
    "    for cand in [\"station\", \"name\", \"station_name\"]:\n",
    "        if cand in df.columns:\n",
    "            rename_map[cand] = \"station\" #pick common columns\n",
    "            break\n",
    "    for cand in [\"line\", \"lines\"]:\n",
    "        if cand in df.columns:\n",
    "            rename_map[cand] = \"line\"\n",
    "            break\n",
    "    for cand in [\"latitude\",\"lat\"]:\n",
    "        if cand in df.columns:\n",
    "            rename_map[cand] = \"lat\"\n",
    "            break\n",
    "    for cand in [\"longitude\",\"lon\",\"lng\",\"long\"]:\n",
    "        if cand in df.columns:\n",
    "            rename_map[cand] = \"lon\"\n",
    "            break\n",
    "    if rename_map:\n",
    "        df = df.rename(columns=rename_map)\n",
    "\n",
    "    if \"station\" in df.columns:\n",
    "        df[\"station\"] = df[\"station\"].astype(str).str.strip().str.title()\n",
    "    if \"line\" in df.columns:\n",
    "        df[\"line\"] = df[\"line\"].astype(str).str.strip().str.title()\n",
    "\n",
    "    for c in [\"lat\",\"lon\"]:\n",
    "        if c in df.columns:\n",
    "            df[c] = pd.to_numeric(df[c], errors=\"coerce\") #lat/lon\n",
    "\n",
    "    keep = [c for c in [\"station\",\"line\",\"lat\",\"lon\",\"borough\",\"zone\"] if c in df.columns]\n",
    "    if keep:\n",
    "        df = df[keep] #keep useful columns\n",
    "\n",
    "    df = df.drop_duplicates().dropna(how=\"all\")\n",
    "    return df\n",
    "\n",
    "stations_clean = clean_stations_csv(RAW_STATIONS_CSV)\n",
    "stations_clean.to_csv(OUT_STATIONS_CLEAN, index=False)\n",
    "print(f\"Wrote {OUT_STATIONS_CLEAN}  rows={len(stations_clean)}\")\n",
    "display(stations_clean.head(10))\n",
    "\n",
    "\n",
    "#cleanin file #3:\n",
    "def clean_tube_performance(xlsx_path: str) -> pd.DataFrame:\n",
    "    xl = pd.ExcelFile(xlsx_path)\n",
    "    frames = []\n",
    "    for s in xl.sheet_names:\n",
    "        df = xl.parse(s, header=0).dropna(how=\"all\").dropna(axis=1, how=\"all\")\n",
    "        if df.shape[1] < 2:\n",
    "            continue\n",
    "        \n",
    "        cols_norm = [re.sub(r\"\\s+\", \" \", str(c).strip().lower()) for c in df.columns]\n",
    "        cmap = {cn: c for cn, c in zip(cols_norm, df.columns)} #normalize headers\n",
    "\n",
    "        line_col = None\n",
    "        for key in [\"line\", \"line name\", \"route\"]:\n",
    "            hit = [cmap[k] for k in list(cmap) if key in k]\n",
    "            if hit:\n",
    "                line_col = hit[0]; break\n",
    "        if line_col is None:\n",
    "            obj = df.select_dtypes(exclude=[np.number]).columns\n",
    "            if len(obj) == 0: \n",
    "                continue\n",
    "            line_col = obj[0] #line column?\n",
    "\n",
    "        date_series = None\n",
    "        for key in [\"period start\",\"start date\",\"date\"]:\n",
    "            hit = [cmap[k] for k in list(cmap) if key in k]\n",
    "            if hit:\n",
    "                date_series = pd.to_datetime(df[hit[0]], errors=\"coerce\"); break\n",
    "        if date_series is None and (\"year\" in cmap and \"month\" in cmap):\n",
    "            y = pd.to_numeric(df[cmap[\"year\"]], errors=\"coerce\")\n",
    "            m = pd.to_numeric(df[cmap[\"month\"]], errors=\"coerce\")\n",
    "            date_series = pd.to_datetime(dict(year=y, month=m, day=1), errors=\"coerce\") #date column?\n",
    "\n",
    "        metric_col = None\n",
    "        for key in [\"excess journey time\",\"ejt\",\"lost customer hours\",\"lch\"]:\n",
    "            hit = [cmap[k] for k in list(cmap) if key in k]\n",
    "            if hit:\n",
    "                metric_col = hit[0]; break\n",
    "        if metric_col is None:\n",
    "            num = df.select_dtypes(include=[np.number]).columns\n",
    "            if len(num) == 0: \n",
    "                continue\n",
    "            var = pd.Series({c: pd.to_numeric(df[c], errors=\"coerce\").var(skipna=True) for c in num})\n",
    "            metric_col = var.sort_values(ascending=False).index[0] #metric column?\n",
    "\n",
    "        tidy = pd.DataFrame({\n",
    "            \"line\": df[line_col].astype(str).str.replace(\" line\",\"\", case=False).str.title().str.strip(),\n",
    "            \"metric\": pd.to_numeric(df[metric_col], errors=\"coerce\")})\n",
    "        if date_series is not None:\n",
    "            tidy[\"date\"] = pd.to_datetime(date_series, errors=\"coerce\").dt.to_period(\"M\").dt.to_timestamp()\n",
    "        else:\n",
    "            tidy[\"date\"] = pd.NaT\n",
    "\n",
    "        tidy = tidy.dropna(subset=[\"line\"]).drop_duplicates()\n",
    "        frames.append(tidy[[\"line\",\"date\",\"metric\"]])\n",
    "\n",
    "    if not frames:\n",
    "        raise RuntimeError(\"Performance workbook: produced no rows. Check file path or sheet contents.\")\n",
    "    merged = pd.concat(frames, ignore_index=True)\n",
    "    # collapse duplicates on same (line,date) by mean\n",
    "    merged[\"date\"] = pd.to_datetime(merged[\"date\"], errors=\"coerce\")\n",
    "    merged = (merged.groupby([\"line\",\"date\"], as_index=False).agg(metric=(\"metric\",\"mean\")))\n",
    "    return merged\n",
    "\n",
    "perf_clean = clean_tube_performance(RAW_PERF_XLSX)\n",
    "perf_clean.to_csv(OUT_PERF_CLEAN, index=False)\n",
    "print(f\"Wrote {OUT_PERF_CLEAN}  rows={len(perf_clean)}\")\n",
    "display(perf_clean.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#paths and imports\n",
    "RAW_DIR = Path(\"data/processed\")  #new cleaned CSVs\n",
    "\n",
    "STATION_FLOW_CSV   = RAW_DIR / \"station_flow_clean_basic.csv\"\n",
    "STATIONS_CLEAN_CSV = RAW_DIR / \"TfL_stations_clean.csv\"\n",
    "PERF_CLEAN_CSV     = RAW_DIR / \"tube_performance_clean_basic.csv\"\n",
    "\n",
    "OUT_DIR = Path(\"data/processed\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "station_flow: (2968, 4) ['year', 'station', 'annual_entries_exits', 'daily_avg_passengers']\n",
      "stations_ref: (436, 2) ['station', 'line']\n",
      "perf_clean: (0, 3) ['line', 'date', 'metric']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>station</th>\n",
       "      <th>annual_entries_exits</th>\n",
       "      <th>daily_avg_passengers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017</td>\n",
       "      <td>Nan</td>\n",
       "      <td>253.0</td>\n",
       "      <td>0.693151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017</td>\n",
       "      <td>625</td>\n",
       "      <td>149150.0</td>\n",
       "      <td>408.630137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017</td>\n",
       "      <td>747</td>\n",
       "      <td>147574.0</td>\n",
       "      <td>404.312329</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   year station  annual_entries_exits  daily_avg_passengers\n",
       "0  2017     Nan                 253.0              0.693151\n",
       "1  2017     625              149150.0            408.630137\n",
       "2  2017     747              147574.0            404.312329"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>station</th>\n",
       "      <th>line</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Acton Town</td>\n",
       "      <td>District, Piccadilly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Barbican</td>\n",
       "      <td>Circle, Hammersmith &amp; City, Metropolitan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Aldgate</td>\n",
       "      <td>Circle, Metropolitan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      station                                      line\n",
       "0  Acton Town                      District, Piccadilly\n",
       "1    Barbican  Circle, Hammersmith & City, Metropolitan\n",
       "2     Aldgate                      Circle, Metropolitan"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>line</th>\n",
       "      <th>date</th>\n",
       "      <th>metric</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [line, date, metric]\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#load + sanity check\n",
    "station_flow   = pd.read_csv(STATION_FLOW_CSV)\n",
    "stations_ref   = pd.read_csv(STATIONS_CLEAN_CSV)\n",
    "perf_clean     = pd.read_csv(PERF_CLEAN_CSV, parse_dates=[\"date\"])\n",
    "\n",
    "print(\"station_flow:\", station_flow.shape, station_flow.columns.tolist())\n",
    "print(\"stations_ref:\", stations_ref.shape, stations_ref.columns.tolist())\n",
    "print(\"perf_clean:\",   perf_clean.shape,   perf_clean.columns.tolist())\n",
    "\n",
    "display(station_flow.head(3))\n",
    "display(stations_ref.head(3))\n",
    "display(perf_clean.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Pandas data cast to numpy dtype of object. Check input data with np.asarray(data).",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m mean_absolute_error, mean_absolute_percentage_error\n\u001b[32m     18\u001b[39m split_idx = \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(y)*\u001b[32m0.8\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(y)>\u001b[32m10\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mmax\u001b[39m(\u001b[32m1\u001b[39m,\u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(y)*\u001b[32m0.7\u001b[39m))\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m res = (\u001b[43mSARIMAX\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m.\u001b[49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseasonal_order\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[32;43m12\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m               \u001b[49m\u001b[43menforce_stationarity\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menforce_invertibility\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     21\u001b[39m        .fit(disp=\u001b[38;5;28;01mFalse\u001b[39;00m))\n\u001b[32m     22\u001b[39m y_hat = res.get_prediction(start=y.index[split_idx], end=y.index[-\u001b[32m1\u001b[39m]).predicted_mean\n\u001b[32m     24\u001b[39m \u001b[38;5;28mprint\u001b[39m({\u001b[33m\"\u001b[39m\u001b[33mMAE\u001b[39m\u001b[33m\"\u001b[39m: mean_absolute_error(y.iloc[split_idx:], y_hat),\n\u001b[32m     25\u001b[39m        \u001b[33m\"\u001b[39m\u001b[33mMAPE\u001b[39m\u001b[33m\"\u001b[39m: mean_absolute_percentage_error(y.iloc[split_idx:], y_hat)})\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sklearn-env/lib/python3.11/site-packages/statsmodels/tsa/statespace/sarimax.py:328\u001b[39m, in \u001b[36mSARIMAX.__init__\u001b[39m\u001b[34m(self, endog, exog, order, seasonal_order, trend, measurement_error, time_varying_regression, mle_regression, simple_differencing, enforce_stationarity, enforce_invertibility, hamilton_representation, concentrate_scale, trend_offset, use_exact_diffuse, dates, freq, missing, validate_specification, **kwargs)\u001b[39m\n\u001b[32m    318\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, endog, exog=\u001b[38;5;28;01mNone\u001b[39;00m, order=(\u001b[32m1\u001b[39m, \u001b[32m0\u001b[39m, \u001b[32m0\u001b[39m),\n\u001b[32m    319\u001b[39m              seasonal_order=(\u001b[32m0\u001b[39m, \u001b[32m0\u001b[39m, \u001b[32m0\u001b[39m, \u001b[32m0\u001b[39m), trend=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    320\u001b[39m              measurement_error=\u001b[38;5;28;01mFalse\u001b[39;00m, time_varying_regression=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    325\u001b[39m              freq=\u001b[38;5;28;01mNone\u001b[39;00m, missing=\u001b[33m'\u001b[39m\u001b[33mnone\u001b[39m\u001b[33m'\u001b[39m, validate_specification=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    326\u001b[39m              **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m328\u001b[39m     \u001b[38;5;28mself\u001b[39m._spec = \u001b[43mSARIMAXSpecification\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    329\u001b[39m \u001b[43m        \u001b[49m\u001b[43mendog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexog\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[43m=\u001b[49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseasonal_order\u001b[49m\u001b[43m=\u001b[49m\u001b[43mseasonal_order\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    330\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrend\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrend\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menforce_stationarity\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menforce_invertibility\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    331\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconcentrate_scale\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconcentrate_scale\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdates\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdates\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfreq\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfreq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    332\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmissing\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmissing\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidate_specification\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalidate_specification\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    333\u001b[39m     \u001b[38;5;28mself\u001b[39m._params = SARIMAXParams(\u001b[38;5;28mself\u001b[39m._spec)\n\u001b[32m    335\u001b[39m     \u001b[38;5;66;03m# Save given orders\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sklearn-env/lib/python3.11/site-packages/statsmodels/tsa/arima/specification.py:446\u001b[39m, in \u001b[36mSARIMAXSpecification.__init__\u001b[39m\u001b[34m(self, endog, exog, order, seasonal_order, ar_order, diff, ma_order, seasonal_ar_order, seasonal_diff, seasonal_ma_order, seasonal_periods, trend, enforce_stationarity, enforce_invertibility, concentrate_scale, trend_offset, dates, freq, missing, validate_specification)\u001b[39m\n\u001b[32m    441\u001b[39m         exog = np.c_[trend_data, exog]\n\u001b[32m    443\u001b[39m \u001b[38;5;66;03m# Create an underlying time series model, to handle endog / exog,\u001b[39;00m\n\u001b[32m    444\u001b[39m \u001b[38;5;66;03m# especially validating shapes, retrieving names, and potentially\u001b[39;00m\n\u001b[32m    445\u001b[39m \u001b[38;5;66;03m# providing us with a time series index\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m446\u001b[39m \u001b[38;5;28mself\u001b[39m._model = \u001b[43mTimeSeriesModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mendog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexog\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdates\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdates\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfreq\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfreq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    447\u001b[39m \u001b[43m                              \u001b[49m\u001b[43mmissing\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmissing\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    448\u001b[39m \u001b[38;5;28mself\u001b[39m.endog = \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m faux_endog \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m._model.endog\n\u001b[32m    449\u001b[39m \u001b[38;5;28mself\u001b[39m.exog = \u001b[38;5;28mself\u001b[39m._model.exog\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sklearn-env/lib/python3.11/site-packages/statsmodels/tsa/base/tsa_model.py:470\u001b[39m, in \u001b[36mTimeSeriesModel.__init__\u001b[39m\u001b[34m(self, endog, exog, dates, freq, missing, **kwargs)\u001b[39m\n\u001b[32m    467\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\n\u001b[32m    468\u001b[39m     \u001b[38;5;28mself\u001b[39m, endog, exog=\u001b[38;5;28;01mNone\u001b[39;00m, dates=\u001b[38;5;28;01mNone\u001b[39;00m, freq=\u001b[38;5;28;01mNone\u001b[39;00m, missing=\u001b[33m\"\u001b[39m\u001b[33mnone\u001b[39m\u001b[33m\"\u001b[39m, **kwargs\n\u001b[32m    469\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m470\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mendog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmissing\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmissing\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    472\u001b[39m     \u001b[38;5;66;03m# Date handling in indexes\u001b[39;00m\n\u001b[32m    473\u001b[39m     \u001b[38;5;28mself\u001b[39m._init_dates(dates, freq)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sklearn-env/lib/python3.11/site-packages/statsmodels/base/model.py:270\u001b[39m, in \u001b[36mLikelihoodModel.__init__\u001b[39m\u001b[34m(self, endog, exog, **kwargs)\u001b[39m\n\u001b[32m    269\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, endog, exog=\u001b[38;5;28;01mNone\u001b[39;00m, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m270\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mendog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    271\u001b[39m     \u001b[38;5;28mself\u001b[39m.initialize()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sklearn-env/lib/python3.11/site-packages/statsmodels/base/model.py:95\u001b[39m, in \u001b[36mModel.__init__\u001b[39m\u001b[34m(self, endog, exog, **kwargs)\u001b[39m\n\u001b[32m     93\u001b[39m missing = kwargs.pop(\u001b[33m'\u001b[39m\u001b[33mmissing\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mnone\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     94\u001b[39m hasconst = kwargs.pop(\u001b[33m'\u001b[39m\u001b[33mhasconst\u001b[39m\u001b[33m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m95\u001b[39m \u001b[38;5;28mself\u001b[39m.data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_handle_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mendog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmissing\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhasconst\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     96\u001b[39m \u001b[43m                              \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     97\u001b[39m \u001b[38;5;28mself\u001b[39m.k_constant = \u001b[38;5;28mself\u001b[39m.data.k_constant\n\u001b[32m     98\u001b[39m \u001b[38;5;28mself\u001b[39m.exog = \u001b[38;5;28mself\u001b[39m.data.exog\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sklearn-env/lib/python3.11/site-packages/statsmodels/base/model.py:135\u001b[39m, in \u001b[36mModel._handle_data\u001b[39m\u001b[34m(self, endog, exog, missing, hasconst, **kwargs)\u001b[39m\n\u001b[32m    134\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_handle_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, endog, exog, missing, hasconst, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m135\u001b[39m     data = \u001b[43mhandle_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mendog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmissing\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhasconst\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    136\u001b[39m     \u001b[38;5;66;03m# kwargs arrays could have changed, easier to just attach here\u001b[39;00m\n\u001b[32m    137\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m kwargs:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sklearn-env/lib/python3.11/site-packages/statsmodels/base/data.py:675\u001b[39m, in \u001b[36mhandle_data\u001b[39m\u001b[34m(endog, exog, missing, hasconst, **kwargs)\u001b[39m\n\u001b[32m    672\u001b[39m     exog = np.asarray(exog)\n\u001b[32m    674\u001b[39m klass = handle_data_class_factory(endog, exog)\n\u001b[32m--> \u001b[39m\u001b[32m675\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mklass\u001b[49m\u001b[43m(\u001b[49m\u001b[43mendog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexog\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmissing\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmissing\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhasconst\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhasconst\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    676\u001b[39m \u001b[43m             \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sklearn-env/lib/python3.11/site-packages/statsmodels/base/data.py:84\u001b[39m, in \u001b[36mModelData.__init__\u001b[39m\u001b[34m(self, endog, exog, missing, hasconst, **kwargs)\u001b[39m\n\u001b[32m     82\u001b[39m     \u001b[38;5;28mself\u001b[39m.orig_endog = endog\n\u001b[32m     83\u001b[39m     \u001b[38;5;28mself\u001b[39m.orig_exog = exog\n\u001b[32m---> \u001b[39m\u001b[32m84\u001b[39m     \u001b[38;5;28mself\u001b[39m.endog, \u001b[38;5;28mself\u001b[39m.exog = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_convert_endog_exog\u001b[49m\u001b[43m(\u001b[49m\u001b[43mendog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexog\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     86\u001b[39m \u001b[38;5;28mself\u001b[39m.const_idx = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     87\u001b[39m \u001b[38;5;28mself\u001b[39m.k_constant = \u001b[32m0\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sklearn-env/lib/python3.11/site-packages/statsmodels/base/data.py:509\u001b[39m, in \u001b[36mPandasData._convert_endog_exog\u001b[39m\u001b[34m(self, endog, exog)\u001b[39m\n\u001b[32m    507\u001b[39m exog = exog \u001b[38;5;28;01mif\u001b[39;00m exog \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m np.asarray(exog)\n\u001b[32m    508\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m endog.dtype == \u001b[38;5;28mobject\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m exog \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m exog.dtype == \u001b[38;5;28mobject\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m509\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mPandas data cast to numpy dtype of object. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    510\u001b[39m                      \u001b[33m\"\u001b[39m\u001b[33mCheck input data with np.asarray(data).\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    511\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()._convert_endog_exog(endog, exog)\n",
      "\u001b[31mValueError\u001b[39m: Pandas data cast to numpy dtype of object. Check input data with np.asarray(data)."
     ]
    }
   ],
   "source": [
    "#monthly performance per line, SARIMA\n",
    " \n",
    "perf = perf_clean.copy()\n",
    "perf[\"date\"] = pd.to_datetime(perf[\"date\"], errors=\"coerce\").dt.to_period(\"M\").dt.to_timestamp()\n",
    "perf = (perf.dropna(subset=[\"line\"])\n",
    "            .groupby([\"line\",\"date\"], as_index=False)\n",
    "            .agg(metric=(\"metric\",\"mean\")))\n",
    " \n",
    "line = \"Jubilee\"  # change to any available line\n",
    "y = (perf[perf[\"line\"]==line]\n",
    "       .set_index(\"date\")[\"metric\"]\n",
    "       .asfreq(\"MS\")\n",
    "       .interpolate(limit_direction=\"both\"))\n",
    " \n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error\n",
    " \n",
    "split_idx = int(len(y)*0.8) if len(y)>10 else max(1,int(len(y)*0.7))\n",
    "res = (SARIMAX(y.iloc[:split_idx], order=(1,1,1), seasonal_order=(1,1,1,12),\n",
    "               enforce_stationarity=False, enforce_invertibility=False)\n",
    "       .fit(disp=False))\n",
    "y_hat = res.get_prediction(start=y.index[split_idx], end=y.index[-1]).predicted_mean\n",
    " \n",
    "print({\"MAE\": mean_absolute_error(y.iloc[split_idx:], y_hat),\n",
    "       \"MAPE\": mean_absolute_percentage_error(y.iloc[split_idx:], y_hat)})\n",
    " \n",
    "y.plot(label=\"actual\", figsize=(9,4)); y_hat.plot(label=\"forecast\")\n",
    "plt.legend(); plt.title(f\"{line} â€“ SARIMA (no exogenous)\"); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "primary-line mapping coverage: 0.0%  (unmapped=100.0%)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>station</th>\n",
       "      <th>annual_entries_exits</th>\n",
       "      <th>daily_avg_passengers</th>\n",
       "      <th>line</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017</td>\n",
       "      <td>Nan</td>\n",
       "      <td>253.0</td>\n",
       "      <td>0.693151</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017</td>\n",
       "      <td>625</td>\n",
       "      <td>149150.0</td>\n",
       "      <td>408.630137</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017</td>\n",
       "      <td>747</td>\n",
       "      <td>147574.0</td>\n",
       "      <td>404.312329</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017</td>\n",
       "      <td>669</td>\n",
       "      <td>121364.0</td>\n",
       "      <td>332.504110</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017</td>\n",
       "      <td>741</td>\n",
       "      <td>121018.0</td>\n",
       "      <td>331.556164</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   year station  annual_entries_exits  daily_avg_passengers line\n",
       "0  2017     Nan                 253.0              0.693151  NaN\n",
       "1  2017     625              149150.0            408.630137  NaN\n",
       "2  2017     747              147574.0            404.312329  NaN\n",
       "3  2017     669              121364.0            332.504110  NaN\n",
       "4  2017     741              121018.0            331.556164  NaN"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "######### STATION LINE MAPPING\n",
    "def primary_line(s):\n",
    "    if pd.isna(s): return np.nan\n",
    "    part = re.split(r\"\\s*(?:/|;|,|&|\\+|\\band\\b)\\s*\", str(s), flags=re.I)[0]  # first token only\n",
    "    return part.strip().title() if part.strip() else np.nan\n",
    " \n",
    "# ensure we have a 'station' column in both frames (fallback to first col if needed)\n",
    "if \"station\" not in station_flow.columns:\n",
    "    station_flow = station_flow.rename(columns={station_flow.columns[0]: \"station\"})\n",
    "if \"station\" not in stations_ref.columns:\n",
    "    stations_ref = stations_ref.rename(columns={stations_ref.columns[0]: \"station\"})\n",
    " \n",
    "# normalise names\n",
    "station_flow[\"station\"] = station_flow[\"station\"].astype(str).str.strip().str.title()\n",
    "stations_ref[\"station\"] = stations_ref[\"station\"].astype(str).str.strip().str.title()\n",
    " \n",
    "# find a 'line'/'lines' column; if none, make NaN (merge will still work)\n",
    "line_col = next((c for c in stations_ref.columns if re.search(r\"\\blines?\\b\", str(c), flags=re.I)), None)\n",
    "stations_ref[\"line\"] = stations_ref[line_col].astype(str) if line_col else np.nan\n",
    " \n",
    "# derive 'primary_line' and build mapping\n",
    "stations_ref[\"primary_line\"] = stations_ref[\"line\"].apply(primary_line)\n",
    "stn_map = (stations_ref[[\"station\",\"primary_line\"]]\n",
    "           .dropna()\n",
    "           .drop_duplicates()\n",
    "           .rename(columns={\"primary_line\":\"line\"}))\n",
    " \n",
    "# merge to attach one line per station\n",
    "sf_with_line = station_flow.merge(stn_map, on=\"station\", how=\"left\")\n",
    " \n",
    "# coverage (pure pandas; forced to float for safe formatting)\n",
    "unmapped = sf_with_line[\"line\"].isna().mean()\n",
    "coverage = 1.0 - unmapped\n",
    "print(f\"primary-line mapping coverage: {float(coverage):.1%}  (unmapped={float(unmapped):.1%})\")\n",
    "display(sf_with_line.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sklearn-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
