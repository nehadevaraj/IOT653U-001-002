{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#microphone check one two what is this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from pathlib import Path\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX #pip install statsmodels\n",
    "from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"openpyxl\")\n",
    "\n",
    "#from thefuzz import process - not needed anymore\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import packaging as p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'#check counts and names\\nimport pandas as pd, re\\nfrom pprint import pprint\\n\\nxl = pd.ExcelFile(\"multi-year-station-entry-and-exit-figures.xlsx\")\\n\\ndef norm(s): \\n    import re\\n    s = str(s).lower().strip()\\n    s = re.sub(r\"[^a-z0-9]+\", \" \", s)\\n    return re.sub(r\"\\\\s+\", \" \", s).strip()\\n\\ndef find_header_row(xl, sheet, scan_rows=60):\\n    raw = xl.parse(sheet, header=None)\\n    for i in range(min(scan_rows, len(raw))):\\n        row = [norm(v) for v in raw.iloc[i].tolist()]\\n        if any(\"station\" in v for v in row) or (any(\"entr\" in v for v in row) and any(\"exit\" in v for v in row)):\\n            return i\\n    return None\\n\\ns = \"2017 Entry & Exit\"\\nhdr = find_header_row(xl, s)\\ndf = xl.parse(s, header=hdr).dropna(how=\"all\").dropna(axis=1, how=\"all\")\\npprint([norm(c) for c in df.columns])'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''#check counts and names\n",
    "import pandas as pd, re\n",
    "from pprint import pprint\n",
    "\n",
    "xl = pd.ExcelFile(\"multi-year-station-entry-and-exit-figures.xlsx\")\n",
    "\n",
    "def norm(s): \n",
    "    import re\n",
    "    s = str(s).lower().strip()\n",
    "    s = re.sub(r\"[^a-z0-9]+\", \" \", s)\n",
    "    return re.sub(r\"\\s+\", \" \", s).strip()\n",
    "\n",
    "def find_header_row(xl, sheet, scan_rows=60):\n",
    "    raw = xl.parse(sheet, header=None)\n",
    "    for i in range(min(scan_rows, len(raw))):\n",
    "        row = [norm(v) for v in raw.iloc[i].tolist()]\n",
    "        if any(\"station\" in v for v in row) or (any(\"entr\" in v for v in row) and any(\"exit\" in v for v in row)):\n",
    "            return i\n",
    "    return None\n",
    "\n",
    "s = \"2017 Entry & Exit\"\n",
    "hdr = find_header_row(xl, s)\n",
    "df = xl.parse(s, header=hdr).dropna(how=\"all\").dropna(axis=1, how=\"all\")\n",
    "pprint([norm(c) for c in df.columns])'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote data/processed/station_flow_clean_basic.csv  rows=2968\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>station</th>\n",
       "      <th>annual_entries_exits</th>\n",
       "      <th>daily_avg_passengers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017</td>\n",
       "      <td>Nan</td>\n",
       "      <td>253.0</td>\n",
       "      <td>0.693151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017</td>\n",
       "      <td>625</td>\n",
       "      <td>149150.0</td>\n",
       "      <td>408.630137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017</td>\n",
       "      <td>747</td>\n",
       "      <td>147574.0</td>\n",
       "      <td>404.312329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017</td>\n",
       "      <td>669</td>\n",
       "      <td>121364.0</td>\n",
       "      <td>332.504110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017</td>\n",
       "      <td>741</td>\n",
       "      <td>121018.0</td>\n",
       "      <td>331.556164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2017</td>\n",
       "      <td>635</td>\n",
       "      <td>113606.0</td>\n",
       "      <td>311.249315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2017</td>\n",
       "      <td>634</td>\n",
       "      <td>112437.0</td>\n",
       "      <td>308.046575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2017</td>\n",
       "      <td>719</td>\n",
       "      <td>93742.0</td>\n",
       "      <td>256.827397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2017</td>\n",
       "      <td>513</td>\n",
       "      <td>112547.0</td>\n",
       "      <td>308.347945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2017</td>\n",
       "      <td>852</td>\n",
       "      <td>86663.0</td>\n",
       "      <td>237.432877</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   year station  annual_entries_exits  daily_avg_passengers\n",
       "0  2017     Nan                 253.0              0.693151\n",
       "1  2017     625              149150.0            408.630137\n",
       "2  2017     747              147574.0            404.312329\n",
       "3  2017     669              121364.0            332.504110\n",
       "4  2017     741              121018.0            331.556164\n",
       "5  2017     635              113606.0            311.249315\n",
       "6  2017     634              112437.0            308.046575\n",
       "7  2017     719               93742.0            256.827397\n",
       "8  2017     513              112547.0            308.347945\n",
       "9  2017     852               86663.0            237.432877"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote data/processed/TfL_stations_clean.csv  rows=436\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>station</th>\n",
       "      <th>line</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Acton Town</td>\n",
       "      <td>District, Piccadilly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Barbican</td>\n",
       "      <td>Circle, Hammersmith &amp; City, Metropolitan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Aldgate</td>\n",
       "      <td>Circle, Metropolitan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Aldgate East</td>\n",
       "      <td>District, Hammersmith &amp; City</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Alperton</td>\n",
       "      <td>Piccadilly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Amersham</td>\n",
       "      <td>Metropolitan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Angel</td>\n",
       "      <td>Northern</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Archway</td>\n",
       "      <td>Northern</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Arnos Grove</td>\n",
       "      <td>Piccadilly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Arsenal</td>\n",
       "      <td>Piccadilly</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        station                                      line\n",
       "0    Acton Town                      District, Piccadilly\n",
       "1      Barbican  Circle, Hammersmith & City, Metropolitan\n",
       "2       Aldgate                      Circle, Metropolitan\n",
       "3  Aldgate East              District, Hammersmith & City\n",
       "4      Alperton                                Piccadilly\n",
       "5      Amersham                              Metropolitan\n",
       "6         Angel                                  Northern\n",
       "7       Archway                                  Northern\n",
       "8   Arnos Grove                                Piccadilly\n",
       "9       Arsenal                                Piccadilly"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote data/processed/tube_performance_clean_basic.csv  rows=0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>line</th>\n",
       "      <th>date</th>\n",
       "      <th>metric</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [line, date, metric]\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#paths\n",
    "RAW_STATION_XLSX   = \"multi-year-station-entry-and-exit-figures.xlsx\"\n",
    "RAW_STATIONS_CSV   = \"TfL_stations.csv\"\n",
    "RAW_PERF_XLSX      = \"tfl-tube-performance.xlsx\"\n",
    "\n",
    "OUT_DIR            = \"data/processed\"\n",
    "OUT_STATION_FLOW   = f\"{OUT_DIR}/station_flow_clean_basic.csv\"\n",
    "OUT_STATIONS_CLEAN = f\"{OUT_DIR}/TfL_stations_clean.csv\"\n",
    "OUT_PERF_CLEAN     = f\"{OUT_DIR}/tube_performance_clean_basic.csv\"\n",
    "\n",
    "Path(OUT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "#cleanin file #1:\n",
    "def clean_station_workbook(xlsx_path: str) -> pd.DataFrame:\n",
    "    xl = pd.ExcelFile(xlsx_path)\n",
    "    frames = []\n",
    "    \n",
    "    target_sheets = [s for s in xl.sheet_names if (\"entry\" in s.lower() and \"exit\" in s.lower())]\n",
    "    for s in target_sheets:\n",
    "        raw = xl.parse(s, header=None) #raw read+ drop empty rows/cols\n",
    "        raw = raw.dropna(how=\"all\").dropna(axis=1, how=\"all\")\n",
    "        if raw.empty:\n",
    "            continue\n",
    "\n",
    "        df = raw.copy()\n",
    "        df.columns = [f\"col{i}\" for i in range(df.shape[1])] #assume first column holds station names\n",
    "        \n",
    "        for c in df.columns[1:]:\n",
    "            df[c] = pd.to_numeric(df[c], errors=\"coerce\") #coerce non first columns to numeric\n",
    "\n",
    "        num_cols = [c for c in df.columns[1:] if np.issubdtype(df[c].dtype, np.number)]\n",
    "        if not num_cols:\n",
    "            continue\n",
    "        q90 = {c: df[c].quantile(0.90) for c in num_cols}\n",
    "        total_col = max(q90, key=lambda k: (q90[k] if pd.notna(q90[k]) else -np.inf)) #choose total as numeric column with highest 90th %\n",
    "\n",
    "        out = pd.DataFrame({\n",
    "            \"station\": df[\"col0\"].astype(str).str.strip().str.title(),\n",
    "            \"annual_entries_exits\": df[total_col]})\n",
    "\n",
    "        out = out.replace({\"\": np.nan, \"nan\": np.nan})\n",
    "        out = out[~out[\"station\"].str.contains(r\"^total\\b|^counts\\b|^grand\\b\", case=False, na=False)]\n",
    "        out = out.dropna(subset=[\"station\", \"annual_entries_exits\"]) #drop junk\n",
    "\n",
    "        if out[\"annual_entries_exits\"].max(skipna=True) < 100_000:\n",
    "            out[\"annual_entries_exits\"] = out[\"annual_entries_exits\"] * 1_000_000 #millions to absolute\n",
    "            \n",
    "        out[\"daily_avg_passengers\"] = out[\"annual_entries_exits\"] / 365.0 #daily average\n",
    "\n",
    "        m = re.search(r\"(20\\d{2})\", s)\n",
    "        out[\"year\"] = int(m.group(1)) if m else np.nan\n",
    "        \n",
    "        frames.append(out[[\"year\",\"station\",\"annual_entries_exits\",\"daily_avg_passengers\"]])\n",
    "\n",
    "    if not frames:\n",
    "        raise RuntimeError(\"Station workbook: produced no rows. Check file path or sheet names.\")\n",
    "\n",
    "    station_flow = pd.concat(frames, ignore_index=True).drop_duplicates()\n",
    "    return station_flow\n",
    "\n",
    "station_flow = clean_station_workbook(RAW_STATION_XLSX)\n",
    "station_flow.to_csv(OUT_STATION_FLOW, index=False)\n",
    "print(f\"Wrote {OUT_STATION_FLOW}  rows={len(station_flow)}\")\n",
    "display(station_flow.head(10))\n",
    "\n",
    "\n",
    "#cleanin file #2:\n",
    "def clean_stations_csv(csv_path: str) -> pd.DataFrame:\n",
    "    df = pd.read_csv(csv_path)\n",
    "    df.columns = [re.sub(r\"_+\", \"_\", re.sub(r\"[^0-9a-zA-Z]+\", \"_\", c)).strip(\"_\").lower() for c in df.columns] #normalize headers\n",
    "    \n",
    "    rename_map = {}\n",
    "    for cand in [\"station\", \"name\", \"station_name\"]:\n",
    "        if cand in df.columns:\n",
    "            rename_map[cand] = \"station\" #pick common columns\n",
    "            break\n",
    "    for cand in [\"line\", \"lines\"]:\n",
    "        if cand in df.columns:\n",
    "            rename_map[cand] = \"line\"\n",
    "            break\n",
    "    for cand in [\"latitude\",\"lat\"]:\n",
    "        if cand in df.columns:\n",
    "            rename_map[cand] = \"lat\"\n",
    "            break\n",
    "    for cand in [\"longitude\",\"lon\",\"lng\",\"long\"]:\n",
    "        if cand in df.columns:\n",
    "            rename_map[cand] = \"lon\"\n",
    "            break\n",
    "    if rename_map:\n",
    "        df = df.rename(columns=rename_map)\n",
    "\n",
    "    if \"station\" in df.columns:\n",
    "        df[\"station\"] = df[\"station\"].astype(str).str.strip().str.title()\n",
    "    if \"line\" in df.columns:\n",
    "        df[\"line\"] = df[\"line\"].astype(str).str.strip().str.title()\n",
    "\n",
    "    for c in [\"lat\",\"lon\"]:\n",
    "        if c in df.columns:\n",
    "            df[c] = pd.to_numeric(df[c], errors=\"coerce\") #lat/lon\n",
    "\n",
    "    keep = [c for c in [\"station\",\"line\",\"lat\",\"lon\",\"borough\",\"zone\"] if c in df.columns]\n",
    "    if keep:\n",
    "        df = df[keep] #keep useful columns\n",
    "\n",
    "    df = df.drop_duplicates().dropna(how=\"all\")\n",
    "    return df\n",
    "\n",
    "stations_clean = clean_stations_csv(RAW_STATIONS_CSV)\n",
    "stations_clean.to_csv(OUT_STATIONS_CLEAN, index=False)\n",
    "print(f\"Wrote {OUT_STATIONS_CLEAN}  rows={len(stations_clean)}\")\n",
    "display(stations_clean.head(10))\n",
    "\n",
    "\n",
    "#cleanin file #3:\n",
    "def clean_tube_performance(xlsx_path: str) -> pd.DataFrame:\n",
    "    xl = pd.ExcelFile(xlsx_path)\n",
    "    frames = []\n",
    "    for s in xl.sheet_names:\n",
    "        df = xl.parse(s, header=0).dropna(how=\"all\").dropna(axis=1, how=\"all\")\n",
    "        if df.shape[1] < 2:\n",
    "            continue\n",
    "        \n",
    "        cols_norm = [re.sub(r\"\\s+\", \" \", str(c).strip().lower()) for c in df.columns]\n",
    "        cmap = {cn: c for cn, c in zip(cols_norm, df.columns)} #normalize headers\n",
    "\n",
    "        line_col = None\n",
    "        for key in [\"line\", \"line name\", \"route\"]:\n",
    "            hit = [cmap[k] for k in list(cmap) if key in k]\n",
    "            if hit:\n",
    "                line_col = hit[0]; break\n",
    "        if line_col is None:\n",
    "            obj = df.select_dtypes(exclude=[np.number]).columns\n",
    "            if len(obj) == 0: \n",
    "                continue\n",
    "            line_col = obj[0] #line column?\n",
    "\n",
    "        date_series = None\n",
    "        for key in [\"period start\",\"start date\",\"date\"]:\n",
    "            hit = [cmap[k] for k in list(cmap) if key in k]\n",
    "            if hit:\n",
    "                date_series = pd.to_datetime(df[hit[0]], errors=\"coerce\"); break\n",
    "        if date_series is None and (\"year\" in cmap and \"month\" in cmap):\n",
    "            y = pd.to_numeric(df[cmap[\"year\"]], errors=\"coerce\")\n",
    "            m = pd.to_numeric(df[cmap[\"month\"]], errors=\"coerce\")\n",
    "            date_series = pd.to_datetime(dict(year=y, month=m, day=1), errors=\"coerce\") #date column?\n",
    "\n",
    "        metric_col = None\n",
    "        for key in [\"excess journey time\",\"ejt\",\"lost customer hours\",\"lch\"]:\n",
    "            hit = [cmap[k] for k in list(cmap) if key in k]\n",
    "            if hit:\n",
    "                metric_col = hit[0]; break\n",
    "        if metric_col is None:\n",
    "            num = df.select_dtypes(include=[np.number]).columns\n",
    "            if len(num) == 0: \n",
    "                continue\n",
    "            var = pd.Series({c: pd.to_numeric(df[c], errors=\"coerce\").var(skipna=True) for c in num})\n",
    "            metric_col = var.sort_values(ascending=False).index[0] #metric column?\n",
    "\n",
    "        tidy = pd.DataFrame({\n",
    "            \"line\": df[line_col].astype(str).str.replace(\" line\",\"\", case=False).str.title().str.strip(),\n",
    "            \"metric\": pd.to_numeric(df[metric_col], errors=\"coerce\")})\n",
    "        if date_series is not None:\n",
    "            tidy[\"date\"] = pd.to_datetime(date_series, errors=\"coerce\").dt.to_period(\"M\").dt.to_timestamp()\n",
    "        else:\n",
    "            tidy[\"date\"] = pd.NaT\n",
    "\n",
    "        tidy = tidy.dropna(subset=[\"line\"]).drop_duplicates()\n",
    "        frames.append(tidy[[\"line\",\"date\",\"metric\"]])\n",
    "\n",
    "    if not frames:\n",
    "        raise RuntimeError(\"Performance workbook: produced no rows. Check file path or sheet contents.\")\n",
    "    merged = pd.concat(frames, ignore_index=True)\n",
    "    # collapse duplicates on same (line,date) by mean\n",
    "    merged[\"date\"] = pd.to_datetime(merged[\"date\"], errors=\"coerce\")\n",
    "    merged = (merged.groupby([\"line\",\"date\"], as_index=False).agg(metric=(\"metric\",\"mean\")))\n",
    "    return merged\n",
    "\n",
    "perf_clean = clean_tube_performance(RAW_PERF_XLSX)\n",
    "perf_clean.to_csv(OUT_PERF_CLEAN, index=False)\n",
    "print(f\"Wrote {OUT_PERF_CLEAN}  rows={len(perf_clean)}\")\n",
    "display(perf_clean.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#paths and imports\n",
    "RAW_DIR = Path(\"data/processed\")  #new cleaned CSVs\n",
    "\n",
    "STATION_FLOW_CSV   = RAW_DIR / \"station_flow_clean_basic.csv\"\n",
    "STATIONS_CLEAN_CSV = RAW_DIR / \"TfL_stations_clean.csv\"\n",
    "PERF_CLEAN_CSV     = RAW_DIR / \"tube_performance_clean_basic.csv\"\n",
    "\n",
    "OUT_DIR = Path(\"data/processed\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "station_flow: (2968, 4) ['year', 'station', 'annual_entries_exits', 'daily_avg_passengers']\n",
      "stations_ref: (436, 2) ['station', 'line']\n",
      "perf_clean: (0, 3) ['line', 'date', 'metric']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>station</th>\n",
       "      <th>annual_entries_exits</th>\n",
       "      <th>daily_avg_passengers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017</td>\n",
       "      <td>Nan</td>\n",
       "      <td>253.0</td>\n",
       "      <td>0.693151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017</td>\n",
       "      <td>625</td>\n",
       "      <td>149150.0</td>\n",
       "      <td>408.630137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017</td>\n",
       "      <td>747</td>\n",
       "      <td>147574.0</td>\n",
       "      <td>404.312329</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   year station  annual_entries_exits  daily_avg_passengers\n",
       "0  2017     Nan                 253.0              0.693151\n",
       "1  2017     625              149150.0            408.630137\n",
       "2  2017     747              147574.0            404.312329"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>station</th>\n",
       "      <th>line</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Acton Town</td>\n",
       "      <td>District, Piccadilly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Barbican</td>\n",
       "      <td>Circle, Hammersmith &amp; City, Metropolitan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Aldgate</td>\n",
       "      <td>Circle, Metropolitan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      station                                      line\n",
       "0  Acton Town                      District, Piccadilly\n",
       "1    Barbican  Circle, Hammersmith & City, Metropolitan\n",
       "2     Aldgate                      Circle, Metropolitan"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>line</th>\n",
       "      <th>date</th>\n",
       "      <th>metric</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [line, date, metric]\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#load + sanity check\n",
    "station_flow   = pd.read_csv(STATION_FLOW_CSV)\n",
    "stations_ref   = pd.read_csv(STATIONS_CLEAN_CSV)\n",
    "perf_clean     = pd.read_csv(PERF_CLEAN_CSV, parse_dates=[\"date\"])\n",
    "\n",
    "print(\"station_flow:\", station_flow.shape, station_flow.columns.tolist())\n",
    "print(\"stations_ref:\", stations_ref.shape, stations_ref.columns.tolist())\n",
    "print(\"perf_clean:\",   perf_clean.shape,   perf_clean.columns.tolist())\n",
    "\n",
    "display(station_flow.head(3))\n",
    "display(stations_ref.head(3))\n",
    "display(perf_clean.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'packaging' has no attribute 'strip'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[72]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      8\u001b[39m stations_ref[\u001b[33m\"\u001b[39m\u001b[33mstation\u001b[39m\u001b[33m\"\u001b[39m] = stations_ref[\u001b[33m\"\u001b[39m\u001b[33mstation\u001b[39m\u001b[33m\"\u001b[39m].astype(\u001b[38;5;28mstr\u001b[39m).str.strip().str.title()\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mline\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m stations_ref.columns: \u001b[38;5;66;03m#explode multi-line cells if line exists or create placeholder\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m     stations_ref[\u001b[33m\"\u001b[39m\u001b[33mline_list\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[43mstations_ref\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mline\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_lines\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m     stations_map = (stations_ref.explode(\u001b[33m\"\u001b[39m\u001b[33mline_list\u001b[39m\u001b[33m\"\u001b[39m, ignore_index=\u001b[38;5;28;01mTrue\u001b[39;00m).rename(columns={\u001b[33m\"\u001b[39m\u001b[33mline_list\u001b[39m\u001b[33m\"\u001b[39m:\u001b[33m\"\u001b[39m\u001b[33mline\u001b[39m\u001b[33m\"\u001b[39m}).drop(columns=[\u001b[33m\"\u001b[39m\u001b[33mline\u001b[39m\u001b[33m\"\u001b[39m], errors=\u001b[33m\"\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sklearn-env/lib/python3.11/site-packages/pandas/core/series.py:4935\u001b[39m, in \u001b[36mSeries.apply\u001b[39m\u001b[34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[39m\n\u001b[32m   4800\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mapply\u001b[39m(\n\u001b[32m   4801\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   4802\u001b[39m     func: AggFuncType,\n\u001b[32m   (...)\u001b[39m\u001b[32m   4807\u001b[39m     **kwargs,\n\u001b[32m   4808\u001b[39m ) -> DataFrame | Series:\n\u001b[32m   4809\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   4810\u001b[39m \u001b[33;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[32m   4811\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   4926\u001b[39m \u001b[33;03m    dtype: float64\u001b[39;00m\n\u001b[32m   4927\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m   4928\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4929\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   4930\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4931\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4932\u001b[39m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[43m=\u001b[49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4933\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4934\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m-> \u001b[39m\u001b[32m4935\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sklearn-env/lib/python3.11/site-packages/pandas/core/apply.py:1422\u001b[39m, in \u001b[36mSeriesApply.apply\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1419\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.apply_compat()\n\u001b[32m   1421\u001b[39m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1422\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sklearn-env/lib/python3.11/site-packages/pandas/core/apply.py:1502\u001b[39m, in \u001b[36mSeriesApply.apply_standard\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1496\u001b[39m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[32m   1497\u001b[39m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[32m   1498\u001b[39m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[32m   1499\u001b[39m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[32m   1500\u001b[39m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[32m   1501\u001b[39m action = \u001b[33m\"\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj.dtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1502\u001b[39m mapped = \u001b[43mobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1503\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m=\u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[32m   1504\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1506\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[32m0\u001b[39m], ABCSeries):\n\u001b[32m   1507\u001b[39m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[32m   1508\u001b[39m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[32m   1509\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m obj._constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index=obj.index)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sklearn-env/lib/python3.11/site-packages/pandas/core/base.py:925\u001b[39m, in \u001b[36mIndexOpsMixin._map_values\u001b[39m\u001b[34m(self, mapper, na_action, convert)\u001b[39m\n\u001b[32m    922\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[32m    923\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m arr.map(mapper, na_action=na_action)\n\u001b[32m--> \u001b[39m\u001b[32m925\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m=\u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sklearn-env/lib/python3.11/site-packages/pandas/core/algorithms.py:1743\u001b[39m, in \u001b[36mmap_array\u001b[39m\u001b[34m(arr, mapper, na_action, convert)\u001b[39m\n\u001b[32m   1741\u001b[39m values = arr.astype(\u001b[38;5;28mobject\u001b[39m, copy=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m   1742\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1743\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1745\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m lib.map_infer_mask(\n\u001b[32m   1746\u001b[39m         values, mapper, mask=isna(values).view(np.uint8), convert=convert\n\u001b[32m   1747\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/lib.pyx:2999\u001b[39m, in \u001b[36mpandas._libs.lib.map_infer\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[72]\u001b[39m\u001b[32m, line 4\u001b[39m, in \u001b[36msplit_lines\u001b[39m\u001b[34m(s)\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m pd.isna(s): \u001b[38;5;28;01mreturn\u001b[39;00m []\n\u001b[32m      3\u001b[39m parts = re.split(\u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[;/,&+]|and\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mstr\u001b[39m(s), flags=re.I) \u001b[38;5;66;03m#common delimiters stackoverflow\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m [p.strip().title() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m \u001b[43mp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstrip\u001b[49m()]\n",
      "\u001b[31mAttributeError\u001b[39m: module 'packaging' has no attribute 'strip'"
     ]
    }
   ],
   "source": [
    "def split_lines(s):\n",
    "    if pd.isna(s): return []\n",
    "    parts = re.split(r\"[;/,&+]|and\", str(s), flags=re.I) #common delimiters stackoverflow\n",
    "    return [p.strip().title() for p in p.strip()] #pip install pylance in terminal\n",
    "\n",
    "#normalise names\n",
    "station_flow[\"station\"] = station_flow[\"station\"].astype(str).str.strip().str.title()\n",
    "stations_ref[\"station\"] = stations_ref[\"station\"].astype(str).str.strip().str.title()\n",
    "\n",
    "if \"line\" in stations_ref.columns: #explode multi-line cells if line exists or create placeholder\n",
    "    stations_ref[\"line_list\"] = stations_ref[\"line\"].apply(split_lines)\n",
    "    stations_map = (stations_ref.explode(\"line_list\", ignore_index=True).rename(columns={\"line_list\":\"line\"}).drop(columns=[\"line\"], errors=\"ignore\"))\n",
    "else:\n",
    "    stations_ref[\"line\"] = np.nan\n",
    "    stations_map = stations_ref.copy()\n",
    "\n",
    "sf_with_line = station_flow.merge(stations_map[[\"station\",\"line\"]], on=\"station\", how=\"left\")#join station_flow and add line\n",
    "\n",
    "unmapped = sf_with_line[\"line\"].isna().mean()\n",
    "print(f\"station->line mapping coverage: {100*(1-unmapped):.1f}%  (unmapped={unmapped:.1%})\")\n",
    "display(sf_with_line.head(5))#coverage check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sklearn-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
