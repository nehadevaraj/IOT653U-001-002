{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#microphone check one two what is this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from pathlib import Path\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX #pip install statsmodels\n",
    "from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"openpyxl\")\n",
    "\n",
    "#from thefuzz import process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'#merging datasets\\nmerged_df = pd.merge(\\n    entry_exit_df,\\n    tube_perf,\\n    left_on=\\'matched_station\\',\\n    right_on=\\'station_name\\',\\n    how=\\'left\\')\\n\\nprint(\"Merged shape:\", merged_df.shape)\\nprint(merged_df.head())'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''tube_perf = pd.read_excel(\"tfl-tube-performance.xlsx\")\n",
    "\n",
    "entry_exit = pd.read_excel(\"multi-year-station-entry-and-exit-figures.xlsx\", sheet_name=None)\n",
    "\n",
    "df_list = []\n",
    "for year, data in entry_exit.items():\n",
    "    data['year'] = year\n",
    "    df_list.append(data)\n",
    "entry_exit_df = pd.concat(df_list, ignore_index=True)'''\n",
    "'''df_list = []\n",
    "for sheet, data in entry_exit.items():\n",
    "    year = re.findall(r'\\d{4}', sheet)\n",
    "    data['year'] = int(year[0]) if year else None\n",
    "    df_list.append(data)\n",
    "\n",
    "entry_exit_df = pd.concat(df_list, ignore_index=True)'''\n",
    "'''tube_perf.columns = tube_perf.columns.str.strip().str.lower().str.replace(\" \", \"_\")\n",
    "entry_exit_df.columns = entry_exit_df.columns.str.strip().str.lower().str.replace(\" \", \"_\")'''\n",
    "'''tube_perf = tube_perf[['name', 'london_underground_performance_data']].rename(columns={'name': 'station_name'})\n",
    "entry_exit_df = entry_exit_df[['station', 'local_authority', 'zone(s)[â€ ]', 'year', 'usage[5]']]\n",
    "\n",
    "#fuzzy matching\n",
    "station_names_perf = tube_perf['station_name'].dropna().unique()\n",
    "entry_exit_df['matched_station'] = entry_exit_df['station'].apply(\n",
    "    lambda x: process.extractOne(x, station_names_perf)[0] if isinstance(x, str) else None)'''\n",
    "'''#merging datasets\n",
    "merged_df = pd.merge(\n",
    "    entry_exit_df,\n",
    "    tube_perf,\n",
    "    left_on='matched_station',\n",
    "    right_on='station_name',\n",
    "    how='left')\n",
    "\n",
    "print(\"Merged shape:\", merged_df.shape)\n",
    "print(merged_df.head())'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['counts 2017 annual entries exits',\n",
      " 'unnamed 1',\n",
      " 'unnamed 2',\n",
      " 'unnamed 3',\n",
      " 'unnamed 4',\n",
      " 'unnamed 5',\n",
      " 'unnamed 6',\n",
      " 'unnamed 7',\n",
      " 'unnamed 8',\n",
      " 'unnamed 9',\n",
      " 'unnamed 10']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd, re\n",
    "from pprint import pprint\n",
    " \n",
    "xl = pd.ExcelFile(\"multi-year-station-entry-and-exit-figures.xlsx\")\n",
    " \n",
    "def norm(s): \n",
    "    import re\n",
    "    s = str(s).lower().strip()\n",
    "    s = re.sub(r\"[^a-z0-9]+\", \" \", s)\n",
    "    return re.sub(r\"\\s+\", \" \", s).strip()\n",
    " \n",
    "def find_header_row(xl, sheet, scan_rows=60):\n",
    "    raw = xl.parse(sheet, header=None)\n",
    "    for i in range(min(scan_rows, len(raw))):\n",
    "        row = [norm(v) for v in raw.iloc[i].tolist()]\n",
    "        if any(\"station\" in v for v in row) or (any(\"entr\" in v for v in row) and any(\"exit\" in v for v in row)):\n",
    "            return i\n",
    "    return None\n",
    " \n",
    "s = \"2017 Entry & Exit\"\n",
    "hdr = find_header_row(xl, s)\n",
    "df = xl.parse(s, header=hdr).dropna(how=\"all\").dropna(axis=1, how=\"all\")\n",
    "pprint([norm(c) for c in df.columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote data/processed/station_flow_clean_basic.csv  rows=2968\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>station</th>\n",
       "      <th>annual_entries_exits</th>\n",
       "      <th>daily_avg_passengers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017</td>\n",
       "      <td>Nan</td>\n",
       "      <td>253.0</td>\n",
       "      <td>0.693151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017</td>\n",
       "      <td>625</td>\n",
       "      <td>149150.0</td>\n",
       "      <td>408.630137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017</td>\n",
       "      <td>747</td>\n",
       "      <td>147574.0</td>\n",
       "      <td>404.312329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017</td>\n",
       "      <td>669</td>\n",
       "      <td>121364.0</td>\n",
       "      <td>332.504110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017</td>\n",
       "      <td>741</td>\n",
       "      <td>121018.0</td>\n",
       "      <td>331.556164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2017</td>\n",
       "      <td>635</td>\n",
       "      <td>113606.0</td>\n",
       "      <td>311.249315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2017</td>\n",
       "      <td>634</td>\n",
       "      <td>112437.0</td>\n",
       "      <td>308.046575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2017</td>\n",
       "      <td>719</td>\n",
       "      <td>93742.0</td>\n",
       "      <td>256.827397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2017</td>\n",
       "      <td>513</td>\n",
       "      <td>112547.0</td>\n",
       "      <td>308.347945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2017</td>\n",
       "      <td>852</td>\n",
       "      <td>86663.0</td>\n",
       "      <td>237.432877</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   year station  annual_entries_exits  daily_avg_passengers\n",
       "0  2017     Nan                 253.0              0.693151\n",
       "1  2017     625              149150.0            408.630137\n",
       "2  2017     747              147574.0            404.312329\n",
       "3  2017     669              121364.0            332.504110\n",
       "4  2017     741              121018.0            331.556164\n",
       "5  2017     635              113606.0            311.249315\n",
       "6  2017     634              112437.0            308.046575\n",
       "7  2017     719               93742.0            256.827397\n",
       "8  2017     513              112547.0            308.347945\n",
       "9  2017     852               86663.0            237.432877"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote data/processed/TfL_stations_clean.csv  rows=436\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>station</th>\n",
       "      <th>line</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Acton Town</td>\n",
       "      <td>District, Piccadilly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Barbican</td>\n",
       "      <td>Circle, Hammersmith &amp; City, Metropolitan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Aldgate</td>\n",
       "      <td>Circle, Metropolitan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Aldgate East</td>\n",
       "      <td>District, Hammersmith &amp; City</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Alperton</td>\n",
       "      <td>Piccadilly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Amersham</td>\n",
       "      <td>Metropolitan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Angel</td>\n",
       "      <td>Northern</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Archway</td>\n",
       "      <td>Northern</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Arnos Grove</td>\n",
       "      <td>Piccadilly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Arsenal</td>\n",
       "      <td>Piccadilly</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        station                                      line\n",
       "0    Acton Town                      District, Piccadilly\n",
       "1      Barbican  Circle, Hammersmith & City, Metropolitan\n",
       "2       Aldgate                      Circle, Metropolitan\n",
       "3  Aldgate East              District, Hammersmith & City\n",
       "4      Alperton                                Piccadilly\n",
       "5      Amersham                              Metropolitan\n",
       "6         Angel                                  Northern\n",
       "7       Archway                                  Northern\n",
       "8   Arnos Grove                                Piccadilly\n",
       "9       Arsenal                                Piccadilly"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote data/processed/tube_performance_clean_basic.csv  rows=0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>line</th>\n",
       "      <th>date</th>\n",
       "      <th>metric</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [line, date, metric]\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from pathlib import Path\n",
    " \n",
    "# ==== EDIT THESE PATHS IF NEEDED ============================================\n",
    "RAW_STATION_XLSX   = \"multi-year-station-entry-and-exit-figures.xlsx\"\n",
    "RAW_STATIONS_CSV   = \"TfL_stations.csv\"\n",
    "RAW_PERF_XLSX      = \"tfl-tube-performance.xlsx\"\n",
    " \n",
    "OUT_DIR            = \"data/processed\"\n",
    "OUT_STATION_FLOW   = f\"{OUT_DIR}/station_flow_clean_basic.csv\"\n",
    "OUT_STATIONS_CLEAN = f\"{OUT_DIR}/TfL_stations_clean.csv\"\n",
    "OUT_PERF_CLEAN     = f\"{OUT_DIR}/tube_performance_clean_basic.csv\"\n",
    " \n",
    "Path(OUT_DIR).mkdir(parents=True, exist_ok=True)\n",
    " \n",
    "# ==== A) CLEAN STATION ENTRY/EXIT WORKBOOK (1 CSV) ==========================\n",
    "def clean_station_workbook(xlsx_path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Super-basic approach for messy, multi-sheet TfL station workbook.\n",
    "    - Reads each 'Entry & Exit' sheet\n",
    "    - Uses first column as station names\n",
    "    - Picks the 'biggest' numeric column as annual entries+exits\n",
    "    - Converts millions -> absolute if needed (simple heuristic)\n",
    "    - Adds 'year' from sheet name\n",
    "    \"\"\"\n",
    "    xl = pd.ExcelFile(xlsx_path)\n",
    "    frames = []\n",
    " \n",
    "    target_sheets = [s for s in xl.sheet_names if (\"entry\" in s.lower() and \"exit\" in s.lower())]\n",
    "    for s in target_sheets:\n",
    "        # raw read, drop empty rows/cols\n",
    "        raw = xl.parse(s, header=None)\n",
    "        raw = raw.dropna(how=\"all\").dropna(axis=1, how=\"all\")\n",
    "        if raw.empty:\n",
    "            continue\n",
    " \n",
    "        # assume first column holds station names (common in these books)\n",
    "        df = raw.copy()\n",
    "        df.columns = [f\"col{i}\" for i in range(df.shape[1])]\n",
    " \n",
    "        # coerce non-first columns to numeric when possible\n",
    "        for c in df.columns[1:]:\n",
    "            df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    " \n",
    "        # choose \"total\" as numeric column with highest 90th percentile\n",
    "        num_cols = [c for c in df.columns[1:] if np.issubdtype(df[c].dtype, np.number)]\n",
    "        if not num_cols:\n",
    "            continue\n",
    "        q90 = {c: df[c].quantile(0.90) for c in num_cols}\n",
    "        total_col = max(q90, key=lambda k: (q90[k] if pd.notna(q90[k]) else -np.inf))\n",
    " \n",
    "        # build tidy\n",
    "        out = pd.DataFrame({\n",
    "            \"station\": df[\"col0\"].astype(str).str.strip().str.title(),\n",
    "            \"annual_entries_exits\": df[total_col]\n",
    "        })\n",
    " \n",
    "        # drop junk rows (totals/headers)\n",
    "        out = out.replace({\"\": np.nan, \"nan\": np.nan})\n",
    "        out = out[~out[\"station\"].str.contains(r\"^total\\b|^counts\\b|^grand\\b\", case=False, na=False)]\n",
    "        out = out.dropna(subset=[\"station\", \"annual_entries_exits\"])\n",
    " \n",
    "        # millions -> absolute (heuristic)\n",
    "        if out[\"annual_entries_exits\"].max(skipna=True) < 100_000:\n",
    "            out[\"annual_entries_exits\"] = out[\"annual_entries_exits\"] * 1_000_000\n",
    " \n",
    "        # daily average\n",
    "        out[\"daily_avg_passengers\"] = out[\"annual_entries_exits\"] / 365.0\n",
    " \n",
    "        # year from sheet name\n",
    "        m = re.search(r\"(20\\d{2})\", s)\n",
    "        out[\"year\"] = int(m.group(1)) if m else np.nan\n",
    " \n",
    "        frames.append(out[[\"year\",\"station\",\"annual_entries_exits\",\"daily_avg_passengers\"]])\n",
    " \n",
    "    if not frames:\n",
    "        raise RuntimeError(\"Station workbook: produced no rows. Check file path or sheet names.\")\n",
    " \n",
    "    station_flow = pd.concat(frames, ignore_index=True).drop_duplicates()\n",
    "    return station_flow\n",
    " \n",
    "station_flow = clean_station_workbook(RAW_STATION_XLSX)\n",
    "station_flow.to_csv(OUT_STATION_FLOW, index=False)\n",
    "print(f\"Wrote {OUT_STATION_FLOW}  rows={len(station_flow)}\")\n",
    "display(station_flow.head(10))\n",
    " \n",
    " \n",
    "# ==== B) CLEAN STATIONS REFERENCE CSV (1 CSV) ===============================\n",
    "def clean_stations_csv(csv_path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Simple clean for the station reference:\n",
    "    - Trim whitespace, Title-case station names\n",
    "    - Standardize column names\n",
    "    - Coerce lat/lon if present\n",
    "    - Drop duplicates\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "    # normalize headers\n",
    "    df.columns = [re.sub(r\"_+\", \"_\", re.sub(r\"[^0-9a-zA-Z]+\", \"_\", c)).strip(\"_\").lower() for c in df.columns]\n",
    " \n",
    "    # pick common columns if present\n",
    "    rename_map = {}\n",
    "    for cand in [\"station\", \"name\", \"station_name\"]:\n",
    "        if cand in df.columns:\n",
    "            rename_map[cand] = \"station\"\n",
    "            break\n",
    "    for cand in [\"line\", \"lines\"]:\n",
    "        if cand in df.columns:\n",
    "            rename_map[cand] = \"line\"\n",
    "            break\n",
    "    for cand in [\"latitude\",\"lat\"]:\n",
    "        if cand in df.columns:\n",
    "            rename_map[cand] = \"lat\"\n",
    "            break\n",
    "    for cand in [\"longitude\",\"lon\",\"lng\",\"long\"]:\n",
    "        if cand in df.columns:\n",
    "            rename_map[cand] = \"lon\"\n",
    "            break\n",
    "    if rename_map:\n",
    "        df = df.rename(columns=rename_map)\n",
    " \n",
    "    if \"station\" in df.columns:\n",
    "        df[\"station\"] = df[\"station\"].astype(str).str.strip().str.title()\n",
    "    if \"line\" in df.columns:\n",
    "        df[\"line\"] = df[\"line\"].astype(str).str.strip().str.title()\n",
    " \n",
    "    # lat/lon if present\n",
    "    for c in [\"lat\",\"lon\"]:\n",
    "        if c in df.columns:\n",
    "            df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    " \n",
    "    # keep only useful columns if present\n",
    "    keep = [c for c in [\"station\",\"line\",\"lat\",\"lon\",\"borough\",\"zone\"] if c in df.columns]\n",
    "    if keep:\n",
    "        df = df[keep]\n",
    " \n",
    "    df = df.drop_duplicates().dropna(how=\"all\")\n",
    "    return df\n",
    " \n",
    "stations_clean = clean_stations_csv(RAW_STATIONS_CSV)\n",
    "stations_clean.to_csv(OUT_STATIONS_CLEAN, index=False)\n",
    "print(f\"Wrote {OUT_STATIONS_CLEAN}  rows={len(stations_clean)}\")\n",
    "display(stations_clean.head(10))\n",
    " \n",
    " \n",
    "# ==== C) CLEAN TUBE PERFORMANCE WORKBOOK (1 CSV) ============================\n",
    "def clean_tube_performance(xlsx_path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Minimal, forgiving:\n",
    "    - For each sheet: drop empty rows/cols\n",
    "    - Choose a 'line' column (first non-numeric or header containing 'line')\n",
    "    - Choose a 'date' column if available (period_start/date OR year+month), else leave NaT\n",
    "    - Choose a 'metric' column: prefer EJT/LCH keywords else numeric with highest variance\n",
    "    - Return one simple CSV: line, date, metric\n",
    "    \"\"\"\n",
    "    xl = pd.ExcelFile(xlsx_path)\n",
    "    frames = []\n",
    "    for s in xl.sheet_names:\n",
    "        df = xl.parse(s, header=0).dropna(how=\"all\").dropna(axis=1, how=\"all\")\n",
    "        if df.shape[1] < 2:\n",
    "            continue\n",
    " \n",
    "        # normalize headers\n",
    "        cols_norm = [re.sub(r\"\\s+\", \" \", str(c).strip().lower()) for c in df.columns]\n",
    "        cmap = {cn: c for cn, c in zip(cols_norm, df.columns)}\n",
    " \n",
    "        # line column\n",
    "        line_col = None\n",
    "        for key in [\"line\", \"line name\", \"route\"]:\n",
    "            hit = [cmap[k] for k in list(cmap) if key in k]\n",
    "            if hit:\n",
    "                line_col = hit[0]; break\n",
    "        if line_col is None:\n",
    "            obj = df.select_dtypes(exclude=[np.number]).columns\n",
    "            if len(obj) == 0: \n",
    "                continue\n",
    "            line_col = obj[0]\n",
    " \n",
    "        # date column (optional)\n",
    "        date_series = None\n",
    "        for key in [\"period start\",\"start date\",\"date\"]:\n",
    "            hit = [cmap[k] for k in list(cmap) if key in k]\n",
    "            if hit:\n",
    "                date_series = pd.to_datetime(df[hit[0]], errors=\"coerce\"); break\n",
    "        if date_series is None and (\"year\" in cmap and \"month\" in cmap):\n",
    "            y = pd.to_numeric(df[cmap[\"year\"]], errors=\"coerce\")\n",
    "            m = pd.to_numeric(df[cmap[\"month\"]], errors=\"coerce\")\n",
    "            date_series = pd.to_datetime(dict(year=y, month=m, day=1), errors=\"coerce\")\n",
    " \n",
    "        # metric column\n",
    "        metric_col = None\n",
    "        for key in [\"excess journey time\",\"ejt\",\"lost customer hours\",\"lch\"]:\n",
    "            hit = [cmap[k] for k in list(cmap) if key in k]\n",
    "            if hit:\n",
    "                metric_col = hit[0]; break\n",
    "        if metric_col is None:\n",
    "            num = df.select_dtypes(include=[np.number]).columns\n",
    "            if len(num) == 0: \n",
    "                continue\n",
    "            var = pd.Series({c: pd.to_numeric(df[c], errors=\"coerce\").var(skipna=True) for c in num})\n",
    "            metric_col = var.sort_values(ascending=False).index[0]\n",
    " \n",
    "        tidy = pd.DataFrame({\n",
    "            \"line\": df[line_col].astype(str).str.replace(\" line\",\"\", case=False).str.title().str.strip(),\n",
    "            \"metric\": pd.to_numeric(df[metric_col], errors=\"coerce\")\n",
    "        })\n",
    "        if date_series is not None:\n",
    "            tidy[\"date\"] = pd.to_datetime(date_series, errors=\"coerce\").dt.to_period(\"M\").dt.to_timestamp()\n",
    "        else:\n",
    "            tidy[\"date\"] = pd.NaT\n",
    " \n",
    "        tidy = tidy.dropna(subset=[\"line\"]).drop_duplicates()\n",
    "        frames.append(tidy[[\"line\",\"date\",\"metric\"]])\n",
    " \n",
    "    if not frames:\n",
    "        raise RuntimeError(\"Performance workbook: produced no rows. Check file path or sheet contents.\")\n",
    "    merged = pd.concat(frames, ignore_index=True)\n",
    "    # collapse duplicates on same (line,date) by mean\n",
    "    merged[\"date\"] = pd.to_datetime(merged[\"date\"], errors=\"coerce\")\n",
    "    merged = (merged\n",
    "              .groupby([\"line\",\"date\"], as_index=False)\n",
    "              .agg(metric=(\"metric\",\"mean\")))\n",
    "    return merged\n",
    " \n",
    "perf_clean = clean_tube_performance(RAW_PERF_XLSX)\n",
    "perf_clean.to_csv(OUT_PERF_CLEAN, index=False)\n",
    "print(f\"Wrote {OUT_PERF_CLEAN}  rows={len(perf_clean)}\")\n",
    "display(perf_clean.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "####paths and imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    " \n",
    "RAW_DIR = Path(\"data/processed\")  # where your new cleaned CSVs were saved\n",
    " \n",
    "STATION_FLOW_CSV   = RAW_DIR / \"station_flow_clean_basic.csv\"\n",
    "STATIONS_CLEAN_CSV = RAW_DIR / \"TfL_stations_clean.csv\"\n",
    "PERF_CLEAN_CSV     = RAW_DIR / \"tube_performance_clean_basic.csv\"\n",
    " \n",
    "OUT_DIR = Path(\"data/processed\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "station_flow: (2968, 4) ['year', 'station', 'annual_entries_exits', 'daily_avg_passengers']\n",
      "stations_ref: (436, 2) ['station', 'line']\n",
      "perf_clean: (0, 3) ['line', 'date', 'metric']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>station</th>\n",
       "      <th>annual_entries_exits</th>\n",
       "      <th>daily_avg_passengers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017</td>\n",
       "      <td>Nan</td>\n",
       "      <td>253.0</td>\n",
       "      <td>0.693151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017</td>\n",
       "      <td>625</td>\n",
       "      <td>149150.0</td>\n",
       "      <td>408.630137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017</td>\n",
       "      <td>747</td>\n",
       "      <td>147574.0</td>\n",
       "      <td>404.312329</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   year station  annual_entries_exits  daily_avg_passengers\n",
       "0  2017     Nan                 253.0              0.693151\n",
       "1  2017     625              149150.0            408.630137\n",
       "2  2017     747              147574.0            404.312329"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>station</th>\n",
       "      <th>line</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Acton Town</td>\n",
       "      <td>District, Piccadilly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Barbican</td>\n",
       "      <td>Circle, Hammersmith &amp; City, Metropolitan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Aldgate</td>\n",
       "      <td>Circle, Metropolitan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      station                                      line\n",
       "0  Acton Town                      District, Piccadilly\n",
       "1    Barbican  Circle, Hammersmith & City, Metropolitan\n",
       "2     Aldgate                      Circle, Metropolitan"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>line</th>\n",
       "      <th>date</th>\n",
       "      <th>metric</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [line, date, metric]\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "######load and sanity check\n",
    "station_flow   = pd.read_csv(STATION_FLOW_CSV)\n",
    "stations_ref   = pd.read_csv(STATIONS_CLEAN_CSV)\n",
    "perf_clean     = pd.read_csv(PERF_CLEAN_CSV, parse_dates=[\"date\"])\n",
    " \n",
    "print(\"station_flow:\", station_flow.shape, station_flow.columns.tolist())\n",
    "print(\"stations_ref:\", stations_ref.shape, stations_ref.columns.tolist())\n",
    "print(\"perf_clean:\",   perf_clean.shape,   perf_clean.columns.tolist())\n",
    " \n",
    "display(station_flow.head(3))\n",
    "display(stations_ref.head(3))\n",
    "display(perf_clean.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "######## station/ line mapping\n",
    "# Normalise names\n",
    "station_flow[\"station\"] = station_flow[\"station\"].astype(str).str.strip().str.title()\n",
    "if \"line\" in stations_ref.columns:\n",
    "    stations_ref[\"line\"] = stations_ref[\"line\"].astype(str)\n",
    "stations_ref[\"station\"] = stations_ref[\"station\"].astype(str).str.strip().str.title()\n",
    " \n",
    "# Split multi-line cells (e.g. \"Circle / H&C\") and explode\n",
    "def split_lines(s):\n",
    "    if pd.isna(s): return []\n",
    "    parts = re.split(r\"[;/,&+]| and \", str(s), flags=re.I)\n",
    "    return [p.strip().title() for p in parts if p.strip()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stations_ref_exploded' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[57]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m#### line x year flow:\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# Join station_flow â†” stations_ref_exploded to attach a line to each station\u001b[39;00m\n\u001b[32m      3\u001b[39m sf_with_line = (\n\u001b[32m      4\u001b[39m     station_flow.merge(\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m         \u001b[43mstations_ref_exploded\u001b[49m[[\u001b[33m\"\u001b[39m\u001b[33mstation\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mline\u001b[39m\u001b[33m\"\u001b[39m]],\n\u001b[32m      6\u001b[39m         on=\u001b[33m\"\u001b[39m\u001b[33mstation\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      7\u001b[39m         how=\u001b[33m\"\u001b[39m\u001b[33mleft\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      8\u001b[39m     )\n\u001b[32m      9\u001b[39m )\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# If some stations donâ€™t have a line mapping, keep them flagged (optional)\u001b[39;00m\n\u001b[32m     12\u001b[39m missing_map = sf_with_line[\u001b[33m\"\u001b[39m\u001b[33mline\u001b[39m\u001b[33m\"\u001b[39m].isna().mean()\n",
      "\u001b[31mNameError\u001b[39m: name 'stations_ref_exploded' is not defined"
     ]
    }
   ],
   "source": [
    "#### line x year flow:\n",
    "# Join station_flow â†” stations_ref_exploded to attach a line to each station\n",
    "sf_with_line = (\n",
    "    station_flow.merge(\n",
    "        stations_ref_exploded[[\"station\",\"line\"]],\n",
    "        on=\"station\",\n",
    "        how=\"left\"\n",
    "    )\n",
    ")\n",
    " \n",
    "# If some stations donâ€™t have a line mapping, keep them flagged (optional)\n",
    "missing_map = sf_with_line[\"line\"].isna().mean()\n",
    "print(f\"Unmapped stations (no line): {missing_map:.1%}\")\n",
    " \n",
    "# Aggregate to line Ã— year\n",
    "line_year = (\n",
    "    sf_with_line\n",
    "      .dropna(subset=[\"year\"])  # year inferred from sheet name earlier\n",
    "      .groupby([\"line\",\"year\"], as_index=False)\n",
    "      .agg(\n",
    "          annual_entries_exits=(\"annual_entries_exits\",\"sum\"),\n",
    "          line_daily_avg=(\"daily_avg_passengers\",\"sum\")\n",
    "      )\n",
    ")\n",
    " \n",
    "# Create a monthly proxy load by dividing the annual by 12 (baseline)\n",
    "line_month_proxy = (\n",
    "    line_year\n",
    "      .assign(month=1)\n",
    "      .assign(date=lambda d: pd.to_datetime(dict(year=d[\"year\"], month=d[\"month\"], day=1)))\n",
    "      .assign(monthly_load=lambda d: d[\"annual_entries_exits\"] / 12.0)\n",
    "      .sort_values([\"line\",\"date\"])\n",
    "      .drop(columns=[\"month\"])\n",
    ")\n",
    " \n",
    "display(line_year.head(10))\n",
    "display(line_month_proxy.head(10))\n",
    " \n",
    "# Save for reuse\n",
    "line_year.to_csv(OUT_DIR / \"line_year_flow.csv\", index=False)\n",
    "line_month_proxy.to_csv(OUT_DIR / \"line_month_load_proxy.csv\", index=False)\n",
    "\n",
    " \n",
    "if \"line\" in stations_ref.columns:\n",
    "    stations_ref[\"line_list\"] = stations_ref[\"line\"].apply(split_lines)\n",
    "    stations_ref_exploded = (\n",
    "        stations_ref\n",
    "        .explode(\"line_list\", ignore_index=True)\n",
    "        .rename(columns={\"line_list\": \"line\"})\n",
    "        .drop(columns=[\"line\"], errors=\"ignore\")\n",
    "    )\n",
    "else:\n",
    "    # If your reference file doesnâ€™t have a line column yet, keep just station (we can still do station-level later)\n",
    "    stations_ref_exploded = stations_ref.copy()\n",
    "    stations_ref_exploded[\"line\"] = np.nan\n",
    " \n",
    "display(stations_ref_exploded.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sklearn-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
